{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Nonlinear Least Squares for Modeling Materials\n",
    "\n",
    "In this lab, we will explore gradient descent on nonlinear least squares.  Suppose we wish to fit a model of the form,\n",
    "\n",
    "     yhat = f(x,w)\n",
    "     \n",
    "where `w` is a vector of paramters and `x` is the vector of predictors.  In nonlinear least squares, we find `w` by minimizing a least-squares function \n",
    "\n",
    "     J(w) = \\sum_i (y_i - f(x_i,w))^2\n",
    "     \n",
    "where the summation is over training samples `(x_i,y_i)`.  In general, this optimization has no closed-form expression.  So gradient descent is widely used.  \n",
    "\n",
    "In this lab, we will implement gradient descent on nonlinear least squares in physical modeling of materials.  Specifically, e we will estimate parameters for expansion of copper as a function of temperature.  In doing this lab, you will learn to:\n",
    "* Set up a nonlinear least squares as an unconstrained optimization function\n",
    "* Compute initial parameter estimates for a simple rational model\n",
    "* Compute the gradients of the least squares objective\n",
    "* Implement gradient descent for minimizing the objective\n",
    "* Implement momentum gradient descent\n",
    "* Visualize the convergence of the algorithm\n",
    "\n",
    "We first import some key packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The NIST agency has an excellent [nonlinear regression website](https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml) that has several datasets for trying nonlinear regression problem.  In this lab, we will use the data from a NIST study involving the thermal expansion of copper. The response variable is the coefficient of thermal expansion, and the predictor variable is temperature in degrees kelvin.  \n",
    "\n",
    "> Hahn, T., NIST (1979), Copper Thermal Expansion Study.  (unpublished}\n",
    "\n",
    "You can download the data as follows.  Note that if you directly use the `pd.read_csv(url, ...)` command you will get an HTTP 403 error.  The code below makes it appear as if the request is coming from a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.itl.nist.gov/div898/handbook/datasets/HAHN1.DAT'\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Simulate a browser request\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Raise an error if the request failed\n",
    "response.raise_for_status()\n",
    "\n",
    "# Read the content into pandas\n",
    "data = StringIO(response.text)\n",
    "df = pd.read_csv(data, sep='\\s+', header=None, skiprows=25)\n",
    "df.columns = [\"x0\", \"y0\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `x0` and `y0` into arrays.  Rescale, `x0` and `y0` to values between `0` and `1` by dividing `x0` and `y0` by the maximum value.  Store the scaled values in vectors `x` and `y`.  The rescaling will help with the conditioning of the fitting.  Plot, `y` vs. `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# x0 = ...\n",
    "# y0 = ...\n",
    "# x = x0/np.max(x0)\n",
    "# y = y0/np.max(y0)\n",
    "# plt.plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem a little more challenging, we will add some noise.  Add random Gaussian noise with mean 0 and std. dev = 0.02 to `y`.  Store the noisy results in `yn`. You can use the `np.random.normal()` function to add Gaussian noise. Plot `yn` vs. `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# yn = y + ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data `(x,yn)` into training and test.  Let `xtr,ytr` be training data and `xts,yts` be the test data.  You can use the `train_test_split` function.  Set `test_size=0.33` so that 1/3 of the samples are held out for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO\n",
    "# xtr, xts, ytr, yts = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Fit for a Rational Model\n",
    "\n",
    "The [NIST website](https://www.itl.nist.gov/div898/strd/nls/data/hahn1.shtml) suggests using a *rational* model of the form,\n",
    "\n",
    "      yhat = (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "      \n",
    "with `d=3`.  The model parameters are `w = [a[0],...,a[d],b[0],...,b[d-1]]` so there are `2d+1` parameters total.    Complete the function below that takes vectors `w` and `x` and predicts a set of values `yhat` using the above model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    \n",
    "    # Get the length\n",
    "    d = (len(w)-1)//2\n",
    "    \n",
    "    # TODO.  Extract a and b from w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "    \n",
    "    \n",
    "    # TODO.  Compute yhat.  You may use the np.polyval function\n",
    "    # But, remember you must flip the order the a and b\n",
    "    # yhat = ...\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit with a nonlinear model, most methods only get convergence to a local minima.  So, you need a good initial condition.  For a rational model, one way to get is to realize that if:\n",
    "\n",
    "\n",
    "    y ~= (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "    \n",
    "Then:\n",
    "\n",
    "    y ~= a[0] + a[1]*x + ... + a[d]*x^d - b[0]*x*y + ... - b[d-1]*x^d*y.\n",
    "    \n",
    "So, we can solve for the the parameters `w = [a,b]` from linear regression of the predictors,\n",
    "\n",
    "    Z[i,:] = [ x[i], ... , x[i]**d, y[i]*x[i], ... , y[i}*x[i]**d ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "\n",
    "# TODO.  Create the transformed feature matrix\n",
    "# Z = ...\n",
    "\n",
    "\n",
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = LinearRegression()\n",
    "# regr.fit(...)\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_ and store the parameter vector in winit\n",
    "# winit = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the predicted values of the `yhat` vs. `x` using your estimated parameter `winit` for 1000 values `x` in `[0,1]`.  On the same plot, plot `yts` vs. `xts`.  You will see that you get a horrible fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = ...\n",
    "# yhat = ...\n",
    "# plot(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the previous fit is poor is that the denominator in `yhat` goes close to zero.   To avoid this problem, we can use Ridge regression, to try to keep the parameters close to zero.  Re-run the fit above with `Ridge` with `alpha = 1e-2`. You should see you get a reasonable, but not perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = Ridge(alpha=1e-2)\n",
    "# regr.fit(...)\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_\n",
    "# winit = ...\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Plot the results as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Loss Function\n",
    "\n",
    "We can now use gradient descent to improve our initial estimate.  Complete the following function to compute\n",
    "\n",
    "    f(w) = 0.5*\\sum_i (y[i] - yhat[i])^2\n",
    "    \n",
    "and `fgrad`, the gradient of `f(w)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(w,x,y):\n",
    "        \n",
    "    \n",
    "    # TODO.  Parse w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "    \n",
    "    \n",
    "    # TODO.  Znum[i,j] = x[i]**j\n",
    "    \n",
    "\n",
    "    # TODO.  Zden[i,j] = x[i]**(j+1)\n",
    "    \n",
    "    \n",
    "    # TODO.  Compute yhat \n",
    "    # Compute the numerator and denominator\n",
    "    \n",
    "    \n",
    "    # TODO.  Compute loss\n",
    "    # f = ...\n",
    "    \n",
    "    \n",
    "    # TODO.  Compute gradients\n",
    "    # fgrad = ...\n",
    "    \n",
    "    \n",
    "    \n",
    "    return f, fgrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the gradient function:\n",
    "* Take `w0=winit` and compute `f0,fgrad0 = feval(w0,xtr,ytr)`\n",
    "* Take `w1` very close to `w0` and compute `f1,fgrad1 = feval(w1,xtr,ytr)`\n",
    "* Verify that `f1-f0` is close to the predicted value based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient descent\n",
    "\n",
    "We will now try to minimize the loss function with gradient descent.  Using the function `feval` defined above, implement gradient descent.  Run gradient descent with a step size of `alpha=1e-6` starting at `w=winit`.  Run it for `nit=10000` iterations.  Compute `fgd[it]`= the objective function on iteration `it`.  Plot `fgd[it]` vs. `it`.  \n",
    "\n",
    "You should see that the training loss decreases, but it still hasn't converged after 10000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = 10000\n",
    "step = 1e-6\n",
    "\n",
    "# TODO\n",
    "# Complete the gradient descent loop\n",
    "\n",
    "# TODO: Plot the loss vs iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to get a faster convergence with adaptive step-size using the Armijo rule. Implement the gradient descent with adaptive step size.  Let `fadapt[it]` be the loss function on iteration `it`.  Plot `fadapt[it]` and `fgd[it]` vs. `it` on the same graph.  You should see a slight improvement, but not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nit = 10000\n",
    "step = 1e-6  # Initial step\n",
    "w0 = winit\n",
    "fadapt = np.zeros(nit)\n",
    "\n",
    "# TODO:  Write the gradient descent loop with adaptive step size\n",
    "# using the Armijo rule\n",
    "f0, fgrad0 = feval(w0,xtr,ytr)\n",
    "\n",
    "\n",
    "# TODO: Plot the loss vs. iteration for both fixed and adaptive step sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using he final estimate for `w` from the adaptive step-size plot the predicted values of the `yhat` vs. `x` usfor 1000 values `x` in `[0,1]`.  On the same plot, plot `yhat` vs. `x` for the initial parameter `w=winit`.  Also, plot `yts` vs. `xts`.  You should see that gradient descent was able to improve the estimat slightly, although the initial estimate was not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  PLot the yhat vs xp for both winit and w0 after convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent\n",
    "\n",
    "This section is bonus.  You will get up to +2 out of 20 points for this.\n",
    "\n",
    "One way to improve gradient descent is to use *momentum*.  In momentum gradient descent, the update rule is:\n",
    "\n",
    "    f, fgrad = feval(w,...)\n",
    "    z = beta*z + fgrad\n",
    "    w = w - step*z\n",
    "    \n",
    "This is similar to gradient descent, except that there is a second order term on the gradient.  Implement this algorithm with `beta = 0.99` and `step=1e-3`.   Compare the convergence of the loss function with gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Implement momentum gradient descent\n",
    "\n",
    "# TODO:  Plot the loss vs. iteration for momentum method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Plot the yhat vs xp for momentum method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
