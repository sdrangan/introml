{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdrangan/introml-soln/blob/master/unit08_svm/lab_emnist_soln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thbGo2OQ5L2X"
      },
      "source": [
        "# Lab: SVMs on Extended MNIST\n",
        "\n",
        "In the [MNIST demo](demo06_mnist_svm.ipynb), we saw how SVMs can be used for the classic MNIST problem of digit recognition. In this lab, we are going to extend the MNIST dataset by adding non-digit letters and see if the classifier can distinguish the digits from the non-digits. All non-digits will be lumped as a single 11-th class. This is a highly simplified version of a full character classification, but will illustrate some concepts.\n",
        "\n",
        "In addition to the concepts in the demo, you will learn:\n",
        "* Select the SVM parameters (`C` and `gamma`) via cross-validation.\n",
        "* Use the `GridSearchCV` method to search for parameters with cross-validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7km_7wr35L2Y"
      },
      "source": [
        "As usual, we download the standard packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YZ7EHMQU5L2Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the EMNIST data\n",
        "\n",
        "The EMNIST data can be downloaded from `pytorch` as follows.  The initial download is 562 MB, so it may take a little while.  On Google colab, it will be in less than a minute.\n",
        "\n",
        "Note:  In an earlier version of the lab, you directly downloaded the data from the [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset), but the dataset appears to have been removed."
      ],
      "metadata": {
        "id": "IU2688VCISS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transform to convert the images to tensors\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Download the EMNIST dataset\n",
        "emnist_dataset = datasets.EMNIST(root='./data', split='balanced', train=True, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "p0fwRf2DK7aC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aaa48f-822d-4f77-8d88-5a8bb6c9957d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:07<00:00, 75.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then convert the data to numpy arrays as follows.  \n",
        "*  `Xtr` is `(ntr,28,28)` the training data array representing `28x28` images with `ntr` samples.\n",
        "* `ytr[i]` is the integer label from 0 to `nclasses-1`.\n",
        "When `ytr[i] = k` it corresponds to the string digit n `classes[k]`."
      ],
      "metadata": {
        "id": "k-XRunSAVsR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr = emnist_dataset.train_data.numpy()\n",
        "ytr = emnist_dataset.targets.numpy()\n",
        "Xts = emnist_dataset.train_data.numpy()\n",
        "yts = emnist_dataset.targets.numpy()\n",
        "classes = emnist_dataset.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOCbf2nqLOOD",
        "outputId": "e6b8de22-c03d-4a0e-de09-e98f033941d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XwFPyWi65L2Z"
      },
      "outputs": [],
      "source": [
        "# TODO:  Print the number of training and test samples\n",
        "#   ntr = ...\n",
        "#   nts = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the classes.  You should see there are classes for:\n",
        "* digits 0 to 9\n",
        "* capital letters `A` to `Z`\n",
        "* some lowercase letters: `a``, `b`, `d`, `e`, `f`, `g`, `h`, `n`, `q`, `r`, `t`\n",
        "\n",
        "Not all lowercase letters are included since they may be too easy to confuse with the uppercase."
      ],
      "metadata": {
        "id": "xZuvzmAUKgWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  Print the classes and the total number of classes\n",
        "#  print(classes)\n",
        "#  nclasses = ..."
      ],
      "metadata": {
        "id": "73JrQ-d6KVO4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPQQni0G5L2a"
      },
      "source": [
        "## Displaying the Characters\n",
        "\n",
        "We will use the function from the demo to plot the characters.  Note that the `label` argument takes either `None` or a string, not a decimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "aXVG_I-b5L2a"
      },
      "outputs": [],
      "source": [
        "def plt_digit(x,label=None):\n",
        "    nrow = 28\n",
        "    ncol = 28\n",
        "    xsq = x.reshape((nrow,ncol))\n",
        "    plt.imshow(xsq.T,  cmap='Greys_r')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    if label:\n",
        "        plt.title(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfQeAHaA5L2a"
      },
      "source": [
        "Plot 8 random samples from the training data.  You can use the `plt_digit` function above with `subplot` to create a nice display.  You may want to size your plot with the `plt.figure(figsize=(10,20))` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gdwhXNf95L2a"
      },
      "outputs": [],
      "source": [
        "# TODO:  Plot 8 random samples from the training data of the digits\n",
        "# Select random digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSEeJM-M5L2a"
      },
      "source": [
        "## Creating a Non-Digit Class\n",
        "\n",
        "\n",
        "Before we begin, we first need to remove all the letters corresponding to `I`, `L` and `O` since they are too similar to the other characters.  Loop over the elements in `remove_list` and delete the rows of `Xtr`, `Xts`, `ytr`, and `yts` corresponding to these characters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_list = ['I', 'L', 'O']\n",
        "\n",
        "# TODO\n",
        "#  Delete rows in data where character corresponds to r\n",
        "#  for r in remove_list:\n",
        "#     ..."
      ],
      "metadata": {
        "id": "mfbwSMcvMSVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM classifiers are VERY SLOW to train.  The training is particularly slow when there are a large number of classes, since the one classifier must be trained for each pair of labels.  To make the problem easier, we are going to lump all of the letters in one class and add that class to the digits.  \n",
        "\n",
        "We can do this by creating a label for the non-digit class `npn_digit_class = 10`.  Then whenever `ytr[i] >= non_digit_class` we set `ytr[i] = non_digit_class`.  We do the same with `yts`.\n"
      ],
      "metadata": {
        "id": "4Djb2JtHiCjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_digit_class = 10\n",
        "\n",
        "# TODO\n",
        "#  ytr = ...\n",
        "#  yts = ..."
      ],
      "metadata": {
        "id": "i_5j1gBninUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, to reduce the training and test time, we will use only a small subset of the training and test data.  Of course, you will not get great results with this small dataset.  But, we can at least illustrate the basic concepts.  Create new training and test data `Xtr1`, `ytr1`, `Xts1` and `yts1` with a randomly selected sample of digits and non-digits shown."
      ],
      "metadata": {
        "id": "Lg1ZOPCejxb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uezhLVl-5L2a"
      },
      "outputs": [],
      "source": [
        "# Number of training and test digits and letters\n",
        "ntr_dig = 5000\n",
        "ntr_let = 1000\n",
        "nts_dig = 5000\n",
        "nts_let = 1000\n",
        "\n",
        "# TODO:\n",
        "#   Xtr1, ytr1 = ...\n",
        "#   Xt21, yt21 = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To confirm that selection worked, print"
      ],
      "metadata": {
        "id": "IA4MDbsrGzGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  Plot 8 random samples from the training data of the digits\n",
        "# Select random digits"
      ],
      "metadata": {
        "id": "UbxM-7uiGyr0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTGrJUJn5L2a"
      },
      "source": [
        "The training data above takes values from 0 to 255.  Rescale the data from -1 to 1.  This will get slightly better performance on the SVM.  Save the scaled data into arrays `Xtr1` and `Xts1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BjqicA5Q5L2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO:  Rescale the data from -1 to 1\n",
        "# Xtr1 = ...\n",
        "# Xts1 = ...\n",
        "\n",
        "\n",
        "# TODO:  Reshape the arrays to (n, 28*28)\n",
        "# Xtr1 = Xtr1.reshape(...)\n",
        "# Xts1 = Xts1.reshape(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0050xEiT5L2a"
      },
      "source": [
        "## Run the SVM classifier\n",
        "\n",
        "First create the SVM classifer. Use an `rbf` classifier with `C=2.8` and `gamma=.0073`. We will look at how to select these parameters laters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "SgEiwTha5L2a"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# TODO:  Create a classifier: a support vector classifier\n",
        "# svc = svm.SVC(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "588g29fg5L2a"
      },
      "source": [
        "Fit the classifier using the scaled training data.  SVMs are insanely slow to train.  But, in this lab, we have kept the training size very small. So, the fitting should take about a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0BpRuo2K5L2b"
      },
      "outputs": [],
      "source": [
        "# TODO:  Fit the classifier on the training data.\n",
        "#   svc.fit(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbqlB2Fj5L2b"
      },
      "source": [
        "Measure the accuracy on the test data.  This too will take another huge amount of time.  Print the accuracy.  If you did everything right, you should get an accuracy of around 89%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wA8hyDqx5L2b"
      },
      "outputs": [],
      "source": [
        "# TODO:  Measure error on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_JZMoB75L2b"
      },
      "source": [
        "The error rate is quite a bit higher than what we got in the digits only case.  Actually, had we done a classifier using all 36 labels instead of collapsing the letters to a single class, the SVM classifier would have done much better.  The reason is that the \"letters\" class is now extremely complex.  \n",
        "\n",
        "Print a confusion matrix.  You should see that the error rate on the \"letters\" class is much higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7IVugUZm5L2b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# TODO:  Print a confusion matrix\n",
        "#  C = confusion_matrix(...)\n",
        "\n",
        "# TODO:  Normalize the confusion matrix so that each row sums to one\n",
        "\n",
        "\n",
        "# TODO:  Print the confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfwmsP8F5L2b"
      },
      "source": [
        "Print:\n",
        "* What fraction of digits are mislabeled as non-digits?  \n",
        "* What fraction of letters are mislabeled as non-digits?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aQz8OIo_5L2b"
      },
      "outputs": [],
      "source": [
        "# TODO:  Print above two error rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5FTDgH45L2b"
      },
      "source": [
        "## Selecting gamma and C via Cross-Validation (Using For-Loops)\n",
        "\n",
        "In the above example, and in the demo, we used a given `gamma` and `C` value.  The selection of the parameters depend on the problem and decent performance of the SVM requires that you select these parameters carefully.  The best way to select the parameters is via cross validation.  Specifically, generally, one tries different values of `gamma` and `C` and selects the pair of values the lowest test error rate.\n",
        "\n",
        "In the code below, we will try to use 3 values for `C` and `gamma` as specified in the arrays `C_test` and `gam_test`.  For each `C` and `gamma` in these arrays, fit a model on the training data and measure the accuracy on the test data.  Then, print the `C` and `gamma` that result in the best accuracy.   \n",
        "\n",
        "Normally, you would try a large number of values for each of the parameters, but an SVM is very slow to train -- even with this small data set.  So, we will just do 3 values of each.  Even then, this could take 30 minutes or so to complete.\n",
        "\n",
        "In this lab, you may do the parameter search over `C` and `gamma` in one of two ways:\n",
        "* This section:  Use for loops and manually search over the parameters.  This is more direct and you will see and control exactly what is happening.\n",
        "* Next section:  Use the `GridSearchCV` method in the `sklearn` package.  This takes a little reading, but once you learn this method, you can more easily use this for complex parameter searches.\n",
        "\n",
        "**You only need to submit the solutions to one of the two sections.**  Pick whichever one you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZQkH4bAm5L2b"
      },
      "outputs": [],
      "source": [
        "C_test = [0.1,1,10]\n",
        "gam_test = [0.001,0.01,0.1]\n",
        "\n",
        "nC = len(C_test)\n",
        "ngam = len(gam_test)\n",
        "acc = np.zeros((nC,ngam))\n",
        "\n",
        "# TODO:  Measure and print the accuracy for each C and gamma value.\n",
        "#  Store the results in acc\n",
        "#\n",
        "#  for i, C in enumerate(C_test):\n",
        "#       for j, gam in enumerate(gam_test):\n",
        "#             ...\n",
        "#             acc[i,j] = ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mZWhzT8H5L2b"
      },
      "outputs": [],
      "source": [
        "# TODO:  Print the accuracy matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j-XaA3mu5L2b"
      },
      "outputs": [],
      "source": [
        "# TODO:  Print the maximum accuracy and the corresponding best C and gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "3Zw5B0Nq5L2e"
      },
      "source": [
        "## Using `GridSearchCV` (Optional Section)\n",
        "\n",
        "\n",
        "In the previous section, you would have likely used `for-loops` to search over the different `C` and `gamma` values.  Since this type of parameter search is so commonly used, `sklearn` has an excellent method `GridSearchCV` that can perform all the operations for you.  In this lab, `GridSearchCV` is not that useful.  But, once you get to more complex parameter searches, the `GridSearchCV` method can save you writing a lot of code.  Importantly, `GridSearchCV` supports parallelization so that fits with different parameters can be fit at the same time.  In this optional section, we will show how to use this method.  \n",
        "\n",
        "**You do not have to do this section, if you did the previous section**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADa1IHns5L2e"
      },
      "source": [
        "The `GridSearchCV` method does the train-test split in addition to the parameter search.  In this case, you have already a fixed train-test split.  So, you first need to combine the train and test data back into a single dataset.\n",
        "\n",
        "Create arrays `X` and `y` from `Xtr1`, `Xts1`, `ytr` and `yts`.  Use `np.vstack` and `np.hstack`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrI7ww9T5L2e"
      },
      "outputs": [],
      "source": [
        "# TODO:  Create combined trained and test data X and y.\n",
        "# X = ...\n",
        "# y = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02-6NeeG5L2e"
      },
      "source": [
        "Normally, `GridSearchCV` will do $K$-fold validation and automatically split the data into training and test in each fold.  But, in this case, we want it to perform only one fold with a specific train-test split.  To do this, we need to do the following:\n",
        "* Create a vector `test_fold` where `test_fold[i] = -1` for the samples `i` in the training data (this indicates that they should not be used as test data in any fold) and `test_fold[i] = 0` for the samples `i` in the test data (this indicates that they should be as test data in fold 0).\n",
        "* Call the method  `ps = sklearn.model_selection.PredefinedSplit(test_fold)` to create a predefined test split object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lSmeWYo-5L2f"
      },
      "outputs": [],
      "source": [
        "# TODO:  Create a pre-defined test split object\n",
        "# import sklearn.model_selection\n",
        "# test_fold = ...\n",
        "# ps = sklearn.model_selection.PredefinedSplit(test_fold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES-PEItR5L2f"
      },
      "source": [
        "Next, read about the `GridSearchCV` method to set up a classifier that includes searching over the parameter grid.  \n",
        "* For the `param_grid` parameter, you will want to create a dictionary to search over `C` and `gamma`.  You will also need to select the `kernel` parameter.\n",
        "* Set `cv = ps` to use the fixed train-test split.\n",
        "* Set `verbose=10` to monitor the progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d5IUFwTn5L2f"
      },
      "outputs": [],
      "source": [
        "# TODO:  Create a GridSearchCV classifier\n",
        "# clf = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBfdIgw35L2f"
      },
      "source": [
        "Fit the classifier using the `fit` method.  The fit method will now search over all the parameters. This will take about 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zt7TZ6Ij5L2f"
      },
      "outputs": [],
      "source": [
        "# TODO: Fit the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biObP3Ot5L2f"
      },
      "source": [
        "Print the `best_score_` and `best_params_` attributes of the classifier to find the best score and parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_B9OUFsA5L2f"
      },
      "outputs": [],
      "source": [
        "# TODO:  Print the best parameter and score of the classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGuqMFKS5L2f"
      },
      "source": [
        "Finally, you can print the test and train score from the `cv_results_['mean_test_score']` and `cv_results_['mean_train_score']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6rf6ekt15L2f"
      },
      "outputs": [],
      "source": [
        "# TODO:  Print the mean test score for each parameter value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Tsx_hKZZ5L2f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}