{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdrangan/introml/blob/master/unit09_neural/neural_inclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq_jd5MhTNVM"
      },
      "source": [
        "# Neural Network In-Class Exercises\n",
        "\n",
        "These are the in-class exercises accompanying the Neural networks unit.  Do these exercises as you progress through the sections in the lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9zzOlGu8TNVN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yec0mpRlTNVN"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Consider a neural network where, for each scalar input `x`, it outputs a predicted value `yhat` as follows:\n",
        "\n",
        "    zh = wh*x + bh\n",
        "    uh = 1/(1 + exp(-zh))   # Sigmoid activation\n",
        "    zo = uh.dot(wo) + bo\n",
        "    yhat = zo               # No activation\n",
        "\n",
        "Using the parameter values below, for scalar inputs `x` in the range of -4 to 8:\n",
        "\n",
        "*  Plot `uh` vs `x`.  Since there are three hidden units, your graph should have three curves\n",
        "*  Plot `yhat` vs `x`.  Since there is one output unit, your graph should have one curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-jfivJ-3TNVN"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-4,8,100)\n",
        "wh = np.array([1,1,1])\n",
        "bh = -np.array([0,2,4])\n",
        "wo = np.array([1,-2,0.5])\n",
        "bo = 0.1\n",
        "\n",
        "# TODO\n",
        "#   uh = ...\n",
        "#   yhat = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_Bw53yTNVO"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "We will now try to a similar neural network to fit a simple nonlinear function. Suppose that we are trying to learn a scalar relation:\n",
        "\n",
        "    y = f0(x)\n",
        "    \n",
        "where `x` and `y` are scalars. Suppose that the true function is `f0(x) = sin(2*pi*x)`, but the estimator does not know this. We get training data as follows.  First plot the training data `xtr`, `ytr`, below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7SYs2qN0TNVO"
      },
      "outputs": [],
      "source": [
        "ntr = 100\n",
        "xtr = np.random.rand(ntr)\n",
        "ytr = np.sin(2*np.pi*xtr) + np.random.normal(0,0.1,ntr)\n",
        "\n",
        "# TODO\n",
        "# plt.plot(...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm8n3OYETNVO"
      },
      "source": [
        "To learn the function, consider a neural network with the same structure as before:\n",
        "\n",
        "    zh = wh*x + bh\n",
        "    uh = 1/(1 + exp(-zh))   # Sigmoid activation\n",
        "    yhat = uh.dot(wo) + bo  # No activation\n",
        "    \n",
        "As we progress through the unit, we will show how to fit the parameters for the network in both the hidden and output layers.  But, to give you an idea of the training, in this exercise, we will fit just the output weights and biases, `wh` and `bh`,  with the hidden weights and biases, `wo` and `bo`, fixed.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GypA6i3kTNVO"
      },
      "source": [
        "For the given parameters in the hidden layer, complete the function `hidden()` below that maps a vector of inputs `x` to produce the matrix of hidden outputs `uh`.  Compute the value of `uh_tr = hidden(xtr,wh,bh)` which is the value of the hidden unit outputs on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iX2EJyhaTNVO"
      },
      "outputs": [],
      "source": [
        "nhid = 4\n",
        "wh = np.ones(nhid)\n",
        "bh = -np.linspace(0,1,nhid)\n",
        "\n",
        "def hidden(x,wh,bh):\n",
        "    # TODO\n",
        "    return uh\n",
        "\n",
        "# TODO\n",
        "#   uh_tr = hidden(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038Np1RETNVO"
      },
      "source": [
        "To fit the parameters for the output layer, we want to find the weights and biases, `wo` and `bo`, such that\n",
        "\n",
        "    ytr ~= uh_tr.dot(wo) + bo\n",
        "\n",
        "We can do this with linear regression:\n",
        "\n",
        "*  Create a `LinearRegression` object `reg`.  \n",
        "*  Fit a linear model with `uh_tr` and `ytr`.  \n",
        "*  Get the coefficients `wo` and `bo` from `reg.coef_` and `reg.intercept_`\n",
        "*  Plot the predictions of the model on `xts` given below.  On the same plot, plot the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2VFZcx1HTNVO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Test points\n",
        "xts      = np.linspace(0,1,100)\n",
        "yts_true = np.sin(2*np.pi*xts)  # f0(xts)\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd7u2H6vTNVO"
      },
      "source": [
        "## Exercise 3 :  Training a Neural Network\n",
        "\n",
        "Now we will try to train the neural network using tensorflow.   In the above example, I manually selected the hidden weights so that you can get a good fit.  But, when you have to train both the hidden and output weights, you will need a few more hidden units.  \n",
        "\n",
        "If you are using `tensorflow`, train a neural network as follows:\n",
        "\n",
        "* If you are using `tenssorflow`, clear the keras session.  Do not do this for `pytorch`.\n",
        "* Create a neural network with 32 hidden units, 1 output unit\n",
        "* Use a sigmoid activation for the hidden layer and a `none` activation for the output layer\n",
        "* Compile with `mean_squared_error` for the `loss` and `metrics`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnoT_MMoTNVO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using pytorch, first load the packages\n"
      ],
      "metadata": {
        "id": "b6XOqKCcWS3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "JuDJPEVCWne_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For PyTorch, next:\n",
        "* Convert the training and test data numpy vector to PyTorch tensors.\n",
        "* Note that you will have to reshape the vectors from `(n)` to `(n,1)`.\n",
        "* Create two  `TensorDataset`'s, `train_dataset` and `test_dataset` from PyTorch tensors\n",
        "* Create `DataLoader` objects from the datasets.  Set the `batch_size` to `100`\n"
      ],
      "metadata": {
        "id": "8afFNm8eUWP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TYxOhQn_TNVO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# TODO:  Convert numpy arrays to PyTorch tensors.\n",
        "# Remember to reshape to (n,1)\n",
        "# xtr_torch = torch.tensor(...)\n",
        "# ytr_torch = torch.tensor(...)\n",
        "# yts_torch = torch.tensor(...)\n",
        "# yts_torch = torch.tensor(...)\n",
        "\n",
        "\n",
        "\n",
        "# Create TensorDatasets\n",
        "#  train_dataset = TensorDataset(...)\n",
        "#  test_dataset = TensorDataset(...)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 100  # Adjust as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next generate a neuranl network class, `Net` with:\n",
        "* 4 hidden units\n",
        "* sigmoid activation\n",
        "* 1 output unit\n",
        "* no output activation (since we are using regression)\n",
        "\n",
        "Instantiate the network"
      ],
      "metadata": {
        "id": "axdb5KW3Vtys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  Define the neural network architecture\n",
        "#  class Net(nn.Module):\n",
        "#     def __init__(self, n_hidden):\n",
        "#         ...\n",
        "#     def forward(self, x):\n",
        "#         ....\n",
        "\n",
        "\n",
        "# TODO: Instantiate the neural network with 4 hidden units\n",
        "#   net = Net(4)\n"
      ],
      "metadata": {
        "id": "eE6bxozCVnlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dAZWuToWxZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# TODO:  Select the loss function and optimizer\n",
        "#  criterion = nn.MSELoss()\n",
        "#  optimizer = optim.Adam(...)\n",
        "\n",
        "# Training loop\n",
        "epochs = 2000\n",
        "lossvals = []\n",
        "\n",
        "# TODO:  Create a training loop\n",
        "#   for epoch in tqdm(range(epochs)):\n",
        "#       ...\n",
        "\n",
        "\n",
        "# TODO:  Make predictions on the test data\n",
        "#    with torch.no_grad():\n",
        "#        ypred = ...\n",
        "\n",
        "\n",
        "# TODO:  Plot the results\n"
      ],
      "metadata": {
        "id": "1rBOWahPXTKo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training loss vs. epoch.  Use `semilogy`."
      ],
      "metadata": {
        "id": "-LjU2j7BZaDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot lossvals vs. epoch\n"
      ],
      "metadata": {
        "id": "DmboxndOZHD_"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}